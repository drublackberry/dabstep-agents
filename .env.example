# API Configuration
BASE_URL=https://api.your-llm-provider.com/v1
API_KEY=sk-your-api-key-here
MODEL=gpt-4o-mini
LLM_GATEWAY=openai

# Hugging Face Token (for dataset access)
HF_TOKEN=hf_your-huggingface-token-here

# SSL Configuration (optional)
# SSL_CERT_FILE=/path/to/your/cert.pem

# Optional: OTLP Endpoint for custom tracing
# When running inside Docker, prefer the host alias:
OTLP_ENDPOINT=http://host.docker.internal:6006/v1/traces
# HOST_DOCKER_INTERNAL=host.docker.internal

# === Some examples ===

# Example: fireworks.ai
# BASE_URL=https://api.fireworks.ai/inference/v1
# API_KEY=fw-your-api-key-here
# MODEL=accounts/fireworks/models/qwen3-235b-a22b-thinking-2507
# LLM_GATEWAY=fireworks_ai

# Example: openai
# BASE_URL=https://api.openai.com/v1
# API_KEY=sk-your-api-key-here
# MODEL=gpt-4o-mini
# LLM_GATEWAY=openai

# Example: local TinyLlama via Ollama
# BASE_URL=http://host.docker.internal:11434
# API_KEY=ollama
# MODEL=tinyllama
# LLM_GATEWAY=ollama
